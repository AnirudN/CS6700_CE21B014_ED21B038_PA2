{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eMDMQ81o0Asx",
        "outputId": "c8282940-d3dd-4524-cf1f-746986e7799d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import gym\n",
        "from tqdm import tqdm_notebook\n",
        "import numpy as np\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "shstWM5M0GmH",
        "outputId": "e21b53a2-fb28-41c6-ee0b-1321f7cdbddd"
      },
      "outputs": [],
      "source": [
        "#discount factor for future utilities\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "gamma = 0.99\n",
        "#number of episodes to run\n",
        "NUM_EPISODES = 10000\n",
        "\n",
        "#max steps per episode\n",
        "MAX_STEPS = 10000\n",
        "\n",
        "\n",
        "\n",
        "#device to run model on\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zUzMP6SW0JiN",
        "outputId": "06aedae5-e4e1-43e7-e111-699613430db8"
      },
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "  def __init__(self, observation_space, action_space,seed,layer_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.input_layer = nn.Linear(observation_space, layer_size)\n",
        "        self.output_layer = nn.Linear(layer_size, action_space)\n",
        "  def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = F.relu(x)\n",
        "        actions = self.output_layer(x)\n",
        "        action_probs = F.softmax(actions, dim=-1)\n",
        "        return action_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tRTcs42v1SnQ",
        "outputId": "b397d7ae-5e5f-4cb2-abd8-a3f633000126"
      },
      "outputs": [],
      "source": [
        "\n",
        "class StateValueNetwork(nn.Module):\n",
        "    def __init__(self, observation_space,seed,layer_size):\n",
        "        super(StateValueNetwork, self).__init__()\n",
        "        self.input_layer = nn.Linear(observation_space, 128)\n",
        "        self.output_layer = nn.Linear(128, 1)\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = F.relu(x)\n",
        "        state_value = self.output_layer(x)\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fjmFbOmX1Zy8",
        "outputId": "525c7f72-7489-425a-c5f2-6ab7e1811235"
      },
      "outputs": [],
      "source": [
        "def process_rewards(rewards,gamma):\n",
        "    G = []\n",
        "    total_r = 0\n",
        "    for r in reversed(rewards):\n",
        "        total_r = r + total_r * DISCOUNT_FACTOR\n",
        "        G.insert(0, total_r)\n",
        "    G = torch.tensor(G)\n",
        "    #G = (G - G.mean())/G.std()\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LgawT7xh3RWt",
        "outputId": "33321fdd-7318-4b7f-cdec-b65159415149"
      },
      "outputs": [],
      "source": [
        "def train_value(G, state_vals, optimizer):\n",
        "    val_loss = F.mse_loss(state_vals, G)\n",
        "    optimizer.zero_grad()\n",
        "    val_loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YGaFs7Q8O_bd",
        "outputId": "0dd4ca14-04e8-4ece-88be-b7566f892a0a"
      },
      "outputs": [],
      "source": [
        "seed_list = [1,42,30,25,17]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xdMZb_CQQhcJ",
        "outputId": "cc06c57c-143d-480a-8737-211e97a4f690"
      },
      "outputs": [],
      "source": [
        "def reset_weights(model):\n",
        "    for layer in model.children():\n",
        "        if hasattr(layer, 'reset_parameters'):\n",
        "            layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l790a35YO6oR",
        "outputId": "bdede630-7890-4bd3-d2f7-fcab8aa25fc5"
      },
      "outputs": [],
      "source": [
        "def avg_over_5_runs(params):\n",
        "  avg_regret = 0\n",
        "  for i in range(5):\n",
        "    regret = 0\n",
        "    seed = seed_list[i]\n",
        "    env = gym.make('CartPole-v1')\n",
        "    #Init network\n",
        "    policy_network = PolicyNetwork(env.observation_space.shape[0], env.action_space.n,seed,params[\"network_size\"])\n",
        "    stateval_network = StateValueNetwork(env.observation_space.shape[0],seed,params[\"network_size\"])\n",
        "    reset_weights(policy_network)\n",
        "    reset_weights(stateval_network)\n",
        "\n",
        "    #Init optimizer\n",
        "    policy_optimizer = optim.Adam(policy_network.parameters(), lr=params[\"LR\"])\n",
        "    stateval_optimizer = optim.Adam(stateval_network.parameters(), lr=params[\"LR\"])\n",
        "\n",
        "    ep = 0\n",
        "    action_space = np.arange(env.action_space.n)\n",
        "    total_rewards1 = []\n",
        "    while ep < NUM_EPISODES:\n",
        "      state = env.reset()\n",
        "      states = []\n",
        "      rewards = []\n",
        "      actions = []\n",
        "      done = False\n",
        "      while done == False:\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)[0]\n",
        "        action_probability = policy_network.forward(state).detach().numpy()\n",
        "        if np.isnan(action_probability).any():\n",
        "          print(\"hi\")\n",
        "          state = env.reset()\n",
        "          states = []\n",
        "          rewards = []\n",
        "          actions = []\n",
        "          done = False\n",
        "          state = torch.from_numpy(state).float().unsqueeze(0)[0]\n",
        "          action_probability = policy_network.forward(state).detach().numpy()\n",
        "\n",
        "        action_probability[np.isnan(action_probability)] = 0.0  # Replace NaN with 0.0\n",
        "\n",
        "        action = np.random.choice(action_space,p=action_probability)\n",
        "        state.detach()\n",
        "        next_state,r,done,_ = env.step(action)\n",
        "        states.append(state)\n",
        "        rewards.append(r)\n",
        "        actions.append(action)\n",
        "        if done :\n",
        "          break\n",
        "        state = next_state\n",
        "      total_rewards1.append(sum(rewards))\n",
        "      G = process_rewards(rewards,gamma)\n",
        "      G = torch.FloatTensor(G)\n",
        "\n",
        "      rewards = torch.FloatTensor(rewards)\n",
        "\n",
        "      policy_optimizer.zero_grad()\n",
        "      state_vals = []\n",
        "\n",
        "      for st in states:\n",
        "          st = st.float().unsqueeze(0)[0]\n",
        "\n",
        "          state_vals.append(stateval_network(st))\n",
        "      state_vals = torch.stack(state_vals).squeeze()\n",
        "      #print(state_vals)\n",
        "      train_value(G, state_vals, stateval_optimizer)\n",
        "\n",
        "      deltas = [gt - val for gt, val in zip(G, state_vals)]\n",
        "      deltas = torch.tensor(deltas)\n",
        "      logprob = [torch.log(policy_network.forward(states[i])) for i in range(len(deltas))]\n",
        "      policy_loss = []\n",
        "      for i in range(len(deltas)):\n",
        "\n",
        "            d = deltas[i]\n",
        "\n",
        "            lp = logprob[i][actions[i]]\n",
        "\n",
        "            policy_loss.append(-d * lp)\n",
        "      policy_optimizer.zero_grad()\n",
        "      #print(policy_loss,len(actions))\n",
        "      sum(policy_loss).backward()\n",
        "      policy_optimizer.step()\n",
        "      avg_rewards = np.mean(total_rewards1[-100:])\n",
        "      regret = regret  + 475 - avg_rewards\n",
        "\n",
        "      ep +=1\n",
        "      if ep%400 == 0:\n",
        "          print(\"Ep:\",ep,\"last 100 episodes reward is  :\",avg_rewards, end=\"\\n\")\n",
        "      if avg_rewards > 475:\n",
        "\n",
        "          break\n",
        "\n",
        "    avg_regret = avg_regret + regret\n",
        "    print(\"problem solved at episode\",ep)\n",
        "\n",
        "  return avg_regret / 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hFewFPxRL2D",
        "outputId": "c5a6d3bf-88db-446f-b825-2307025dcd88"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-optimize\n",
        "from skopt import gp_minimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "08yEQcClQUdM",
        "outputId": "7683ba09-55e4-4e81-f487-46ef70c36e3d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "bounds = [(1e-4, 1e-2)]\n",
        "\n",
        "# Run the Bayesian optimization\n",
        "res = gp_minimize(avg_over_5_runs, bounds, n_calls=10, random_state=0)\n",
        "print(res)\n",
        "# Print the best parameters\n",
        "print(f\"Best parameters: learning rate = {res.x[0]}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RHsm-rqOYJo"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRU0Q8qMFqhQ",
        "outputId": "0a89240d-34ae-496f-974d-f51a9f7dac31"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkhB06wXFfMf",
        "outputId": "21c16e85-6bd2-4042-cd07-44b093cffe98"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ff5ea90cb6134fbdaa895579ff30a40b",
            "a2ec2b8ebd03402b857fc4703b29037e",
            "c7b426e81e1f4a1783a8abb05a5f3d48",
            "d339c21eb44d471e8cdc6596909c822e"
          ]
        },
        "id": "_fYqnTS4FqJN",
        "outputId": "f9b9f998-0c7a-4ec6-9810-3993ea8a4fff"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "def main():\n",
        "    wandb.init(project=\"RLA2cartepole-withbase\")\n",
        "    #agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0, config=wandb.config)\n",
        "    score = avg_over_5_runs(wandb.config)\n",
        "    wandb.log({\"score\": score})\n",
        "\n",
        "# 2: Define the search space\n",
        "sweep_configuration = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"goal\": \"minimize\", \"name\": \"score\"},\n",
        "    \"parameters\": {\n",
        "        \"LR\": {\"max\": 1e-2, \"min\": 1e-5},\n",
        "        \"network_size\": {\"values\": [64,256,128]},\n",
        "    },\n",
        "}\n",
        "\n",
        "# 3: Start the sweep\n",
        "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"RLA2cartepole-withbase\")\n",
        "\n",
        "wandb.agent(sweep_id, function=main, count=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_t7RS5cGL5cq",
        "outputId": "ee4474dd-3536-44dd-d3a5-b8957715b975"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "plt.plot(total_rewards)\n",
        "plt.plot()\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.show() '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6prRU1JUOk5h",
        "outputId": "e61d27ac-bcca-4a32-e4ba-042e1884518c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
